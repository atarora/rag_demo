{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c451762f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Please add your Open AI licence Key here\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] =\"sk-XXX\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36f5076",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using https://www.trychroma.com/\n",
    "## Using langchain\n",
    "## Plenty of integrations https://python.langchain.com/docs/modules/data_connection/document_loaders/integrations/arxiv\n",
    "## Using openai gpt-3.5 model\n",
    "## Idea respectfully taken from https://github.com/devsentient/examples/blob/main/LLMs/confluence_app/confluenceQA.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32f1a949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "EMB_OPENAI_ADA = \"text-embedding-ada-002\"\n",
    "LLM_OPENAI_GPT35 = \"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d916cb5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain==0.0.189 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (0.0.189)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from langchain==0.0.189) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from langchain==0.0.189) (2.0.17)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from langchain==0.0.189) (3.8.4)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from langchain==0.0.189) (4.0.2)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from langchain==0.0.189) (0.5.7)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from langchain==0.0.189) (2.8.4)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from langchain==0.0.189) (1.22.4)\n",
      "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from langchain==0.0.189) (1.2.4)\n",
      "Requirement already satisfied: pydantic<2,>=1 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from langchain==0.0.189) (1.10.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from langchain==0.0.189) (2.28.2)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from langchain==0.0.189) (8.2.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.189) (21.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.189) (2.0.12)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.189) (6.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.189) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.189) (1.3.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.189) (1.2.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.189) (3.19.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.189) (1.5.1)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.189) (0.8.0)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from pydantic<2,>=1->langchain==0.0.189) (4.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.0.189) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.0.189) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.0.189) (2021.10.8)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.189) (21.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.189) (1.0.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from packaging>=17.0->marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.189) (3.0.7)\n",
      "Collecting chromadb==0.3.25\n",
      "  Using cached chromadb-0.3.25-py3-none-any.whl (86 kB)\n",
      "Requirement already satisfied: pandas>=1.3 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from chromadb==0.3.25) (1.3.5)\n",
      "Requirement already satisfied: requests>=2.28 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from chromadb==0.3.25) (2.28.2)\n",
      "Requirement already satisfied: pydantic>=1.9 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from chromadb==0.3.25) (1.10.2)\n",
      "Collecting hnswlib>=0.7 (from chromadb==0.3.25)\n",
      "  Using cached hnswlib-0.7.0-cp310-cp310-macosx_11_0_arm64.whl\n",
      "Collecting clickhouse-connect>=0.5.7 (from chromadb==0.3.25)\n",
      "  Using cached clickhouse_connect-0.6.4-cp310-cp310-macosx_11_0_arm64.whl (231 kB)\n",
      "Collecting duckdb>=0.7.1 (from chromadb==0.3.25)\n",
      "  Using cached duckdb-0.8.1-cp310-cp310-macosx_11_0_arm64.whl (12.6 MB)\n",
      "Collecting fastapi>=0.85.1 (from chromadb==0.3.25)\n",
      "  Using cached fastapi-0.98.0-py3-none-any.whl (56 kB)\n",
      "Requirement already satisfied: uvicorn[standard]>=0.18.3 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from chromadb==0.3.25) (0.18.3)\n",
      "Requirement already satisfied: numpy>=1.21.6 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from chromadb==0.3.25) (1.22.4)\n",
      "Collecting posthog>=2.4.0 (from chromadb==0.3.25)\n",
      "  Using cached posthog-3.0.1-py2.py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from chromadb==0.3.25) (1.14.1)\n",
      "Collecting tokenizers>=0.13.2 (from chromadb==0.3.25)\n",
      "  Using cached tokenizers-0.13.3-cp310-cp310-macosx_12_0_arm64.whl (3.9 MB)\n",
      "Collecting tqdm>=4.65.0 (from chromadb==0.3.25)\n",
      "  Using cached tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "Collecting typing-extensions>=4.5.0 (from chromadb==0.3.25)\n",
      "  Using cached typing_extensions-4.7.0-py3-none-any.whl (33 kB)\n",
      "Collecting overrides>=7.3.1 (from chromadb==0.3.25)\n",
      "  Using cached overrides-7.3.1-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: certifi in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.25) (2021.10.8)\n",
      "Requirement already satisfied: urllib3>=1.26 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.25) (1.26.16)\n",
      "Requirement already satisfied: pytz in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.25) (2021.3)\n",
      "Collecting zstandard (from clickhouse-connect>=0.5.7->chromadb==0.3.25)\n",
      "  Using cached zstandard-0.21.0-cp310-cp310-macosx_11_0_arm64.whl (364 kB)\n",
      "Requirement already satisfied: lz4 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.25) (4.0.2)\n",
      "Collecting starlette<0.28.0,>=0.27.0 (from fastapi>=0.85.1->chromadb==0.3.25)\n",
      "  Using cached starlette-0.27.0-py3-none-any.whl (66 kB)\n",
      "Requirement already satisfied: coloredlogs in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.3.25) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.3.25) (23.3.3)\n",
      "Requirement already satisfied: packaging in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.3.25) (21.3)\n",
      "Requirement already satisfied: protobuf in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.3.25) (4.23.2)\n",
      "Requirement already satisfied: sympy in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.3.25) (1.11.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from pandas>=1.3->chromadb==0.3.25) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.3.25) (1.16.0)\n",
      "Requirement already satisfied: monotonic>=1.5 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.3.25) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.3.25) (1.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from requests>=2.28->chromadb==0.3.25) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from requests>=2.28->chromadb==0.3.25) (3.3)\n",
      "Requirement already satisfied: click>=7.0 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.25) (8.1.3)\n",
      "Requirement already satisfied: h11>=0.8 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.25) (0.12.0)\n",
      "Requirement already satisfied: httptools>=0.4.0 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.25) (0.5.0)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.25) (0.21.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.25) (6.0)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.25) (0.17.0)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb==0.3.25)\n",
      "  Using cached watchfiles-0.19.0-cp37-abi3-macosx_11_0_arm64.whl (388 kB)\n",
      "Requirement already satisfied: websockets>=10.0 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.25) (10.3)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from starlette<0.28.0,>=0.27.0->fastapi>=0.85.1->chromadb==0.3.25) (3.6.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb==0.3.25) (10.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from packaging->onnxruntime>=1.14.1->chromadb==0.3.25) (3.0.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb==0.3.25) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi>=0.85.1->chromadb==0.3.25) (1.3.0)\n",
      "Installing collected packages: tokenizers, duckdb, zstandard, typing-extensions, tqdm, overrides, hnswlib, watchfiles, starlette, posthog, clickhouse-connect, fastapi, chromadb\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.12.1\n",
      "    Uninstalling tokenizers-0.12.1:\n",
      "      Successfully uninstalled tokenizers-0.12.1\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.3.0\n",
      "    Uninstalling typing_extensions-4.3.0:\n",
      "      Successfully uninstalled typing_extensions-4.3.0\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.62.3\n",
      "    Uninstalling tqdm-4.62.3:\n",
      "      Successfully uninstalled tqdm-4.62.3\n",
      "  Attempting uninstall: starlette\n",
      "    Found existing installation: starlette 0.19.1\n",
      "    Uninstalling starlette-0.19.1:\n",
      "      Successfully uninstalled starlette-0.19.1\n",
      "  Attempting uninstall: fastapi\n",
      "    Found existing installation: fastapi 0.84.0\n",
      "    Uninstalling fastapi-0.84.0:\n",
      "      Successfully uninstalled fastapi-0.84.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "nbgrader 0.6.2 requires sqlalchemy<1.4, but you have sqlalchemy 2.0.17 which is incompatible.\n",
      "onnx 1.13.1 requires protobuf<4,>=3.20.2, but you have protobuf 4.23.2 which is incompatible.\n",
      "qdrant-client 1.2.0 requires typing-extensions<4.6.0,>=4.0.0, but you have typing-extensions 4.7.0 which is incompatible.\n",
      "ydata-profiling 4.1.2 requires tqdm<4.65,>=4.48.2, but you have tqdm 4.65.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed chromadb-0.3.25 clickhouse-connect-0.6.4 duckdb-0.8.1 fastapi-0.98.0 hnswlib-0.7.0 overrides-7.3.1 posthog-3.0.1 starlette-0.27.0 tokenizers-0.13.3 tqdm-4.65.0 typing-extensions-4.7.0 watchfiles-0.19.0 zstandard-0.21.0\n",
      "Requirement already satisfied: openai==0.27.6 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (0.27.6)\n",
      "Requirement already satisfied: requests>=2.20 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from openai==0.27.6) (2.28.2)\n",
      "Requirement already satisfied: tqdm in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from openai==0.27.6) (4.65.0)\n",
      "Requirement already satisfied: aiohttp in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from openai==0.27.6) (3.8.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from requests>=2.20->openai==0.27.6) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from requests>=2.20->openai==0.27.6) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from requests>=2.20->openai==0.27.6) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from requests>=2.20->openai==0.27.6) (2021.10.8)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from aiohttp->openai==0.27.6) (21.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from aiohttp->openai==0.27.6) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from aiohttp->openai==0.27.6) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from aiohttp->openai==0.27.6) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from aiohttp->openai==0.27.6) (1.3.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from aiohttp->openai==0.27.6) (1.2.0)\n",
      "Requirement already satisfied: pytesseract==0.3.10 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (0.3.10)\n",
      "Requirement already satisfied: packaging>=21.3 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from pytesseract==0.3.10) (21.3)\n",
      "Requirement already satisfied: Pillow>=8.0.0 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from pytesseract==0.3.10) (9.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from packaging>=21.3->pytesseract==0.3.10) (3.0.7)\n",
      "Requirement already satisfied: beautifulsoup4==4.12.2 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from beautifulsoup4==4.12.2) (2.3.2.post1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: atlassian-python-api==3.38.0 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (3.38.0)\n",
      "Requirement already satisfied: deprecated in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from atlassian-python-api==3.38.0) (1.2.14)\n",
      "Requirement already satisfied: requests in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from atlassian-python-api==3.38.0) (2.28.2)\n",
      "Requirement already satisfied: six in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from atlassian-python-api==3.38.0) (1.16.0)\n",
      "Requirement already satisfied: oauthlib in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from atlassian-python-api==3.38.0) (3.2.2)\n",
      "Requirement already satisfied: requests-oauthlib in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from atlassian-python-api==3.38.0) (1.3.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from deprecated->atlassian-python-api==3.38.0) (1.15.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from requests->atlassian-python-api==3.38.0) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from requests->atlassian-python-api==3.38.0) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from requests->atlassian-python-api==3.38.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from requests->atlassian-python-api==3.38.0) (2021.10.8)\n",
      "Requirement already satisfied: tiktoken==0.4.0 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (0.4.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from tiktoken==0.4.0) (2022.9.13)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from tiktoken==0.4.0) (2.28.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken==0.4.0) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken==0.4.0) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken==0.4.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken==0.4.0) (2021.10.8)\n",
      "Requirement already satisfied: lxml==4.9.2 in /Users/atitaarora/.pyenv/versions/3.10.2/lib/python3.10/site-packages (4.9.2)\n"
     ]
    }
   ],
   "source": [
    "## Installations \n",
    "!pip install langchain==0.0.189\n",
    "!pip install chromadb==0.3.25\n",
    "!pip install openai==0.27.6\n",
    "!pip install pytesseract==0.3.10\n",
    "!pip install beautifulsoup4==4.12.2\n",
    "!pip install atlassian-python-api==3.38.0\n",
    "!pip install tiktoken==0.4.0\n",
    "!pip install lxml==4.9.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c57e04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import ConfluenceLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter, TokenTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3adbcb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5de48c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=LLM_OPENAI_GPT35, temperature=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5a947e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Used to extract plain text from the html page https://quepid.com/docs/\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "def extract_text_from(url):\n",
    "    html = requests.get(url).text\n",
    "    soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "    text = soup.get_text()\n",
    "\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    return '\\n'.join(line for line in lines if line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88e1d954",
   "metadata": {},
   "outputs": [],
   "source": [
    "## will go in config file in app\n",
    "\n",
    "config = {\"persist_directory\":\"./chroma_db/\",\n",
    "          \"doc_url\":\"https://quepid.com/docs/\"\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75df0049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documentation | Quepid\n",
      "Toggle navigation\n",
      "Docs\n",
      "Tutorials\n",
      "Documentation\n",
      "Return to Quepid\n",
      "Back to top\n",
      "Getting Started\n",
      "Welcome to Quepid, your search relevancy toolkit. Getting started is easy and we will walk you through the concepts and features to get you setup and tuning relevance in no time.\n",
      "Core Concepts\n",
      "First, let's walk through a few of the core concepts in Quepid.\n",
      "Case:\n",
      "A case refers to all of the queries and relevance tuning settings for a single search engine. If you want to work with multiple instances of Solr or Elasticsearch, you must create a separate case for each one.\n",
      "Query:\n",
      "Within a case, queries are the keywords or other search criteria and their corresponding set of results that will be rated to determine the overall score of a case.\n",
      "Rating:\n",
      "Ratings are the numerical values given to a result that indicates how relevant a particular search result is for the query. How each rating is interpreted depends on the scorer used for the query (or case), but usually the higher the number the more relevant the result is.\n",
      "Result:\n",
      "Within a query are the individual results, which are rated to determine the cumulative score of a query. Sometimes results are also referred to as documents (or docs).\n",
      "Scorer:\n",
      "A scorer refers to the scale used to rate query results. Quepid ships with several classical relevance scorers such as AP, RR, CG, DCG, and NDCG, as well as the ability to create custom scorers.\n",
      "Snapshot:\n",
      "A snapshot is a capture of all your queries and ratings at a point in time. It is important to take regular snapshots of your case to use as benchmarks and ensure that your search relevancy is improving.\n",
      "Team:\n",
      "A team refers to a group of individual users who can view and share cases and custom scorers.\n",
      "Try:\n",
      "A try is a saved iteration of a case's settings. Quepid is a developer tool and we expect developers to constantly tweak the settings, and sometimes you would want to go back to a previous iteration that had better results, so we've made that an integral part of Quepid.\n",
      "Creating an Account\n",
      "All you need is an email address to sign up for free.\n",
      "Quick Start Wizard\n",
      "The \"Quick Start Wizard\", which launches on first login or any time you create a new case, will guide you through configuring your Solr or Elasticsearch instance and setting up your case.\n",
      "What you need to complete the wizard\n",
      "A Solr or Elasticsearch instance that is accessible from your browser\n",
      "A list of desired fields from your search engine\n",
      "*Note: You can use the demo Solr or Elasticsearch instances provided to test Quepid\n",
      "Naming Your Case\n",
      "First you will be required to enter a case name. Select something descriptive to distinguish between your cases.\n",
      "Connecting your Search Engine\n",
      "Quepid does not require any installation on your server. All you need to do is indicate which search engine you are working with, and provide the URL to your search engine. Currently Solr and Elasticsearch are supported.\n",
      "You will be able to update this URL at any point in the settings. If you want to try Quepid out without providing your data, feel free to leave the default collection selected.\n",
      "Selecting Fields to Display\n",
      "Specify the title field to use for display purposes. This typically corresponds to the field you use as a clickable title in your application while displaying search results. The application provides an autosuggest list of potential fields, but you are free to use any field in you search engine.\n",
      "The ID field is the only other required field to use Quepid. This should correspond to the unique identifier specified in your schema and should be a number or string, but not a URL. The ID is the only field stored in Quepid, so your sensitive data will remain securely on your servers.\n",
      "You can select additional fields if you would like to see other data in your Quepid results. You can also specify images with the following syntax: thumb:field_name and video/audio with media:field_name.  Fields that have a value that starts with http will be turned into links.\n",
      "Adding Queries\n",
      "At this point the required setup steps are complete. You have the option to provide some initial queries, but you can also add queries at any time in the application. It is helpful to add at least one query here so that your results will be populated when the application initially loads.\n",
      "Finishing the Wizard\n",
      "That's it! Quepid is configured and you can now proceed to exploring the Quepid interface.\n",
      "Exploring the Interface\n",
      "Now that we have the basic setup out of the way, let's take a look at the major elements of the Quepid interface.\n",
      "1. Relevancy Case Dropdown-\n",
      "The relevancy case dropdown is where you will select a case to work on and create new cases.\n",
      "2. Case Information-\n",
      "This area displays summary information of the current case, including case name, the average score of all queries, and the current try, or iteration in the relevancy dashboard history.\n",
      "3. Case Actions-\n",
      "This area displays available actions for the current case, including selecting a scorer, creating and comparing snapshots, sharing, and toggling the relevancy tuning panel.\n",
      "4. Queries-\n",
      "Here you can add queries to the current case and view existing queries. Click on the arrow to the right to view and rate results for each individual query.\n",
      "5. Relevancy Tuning Panel-\n",
      "The relevancy tuning panel allows you to modify search relevancy settings and tune your searches. This panel is closed by default and can be opened in the case actions section.\n",
      "6. Quepid Support-\n",
      "All pages of the Quepid application include the Quepid support chat window. Click the icon in the bottom right hand of your screen and our search relevancy experts will assist you with any issues or questions you may have.\n",
      "Cases\n",
      "In this section, we will review working with cases in Quepid. If you've logged into the application and gone through the \"Quick Start Wizard\", then you have already created your first case.\n",
      "Creating a New Case\n",
      "If you would like to create an addition case, you can do so under the Relevancy Cases dropdown. Clicking on \"Create a case\" will launch the \"Quick Start Wizard\" and walk you through creating another case.\n",
      "Importing/Exporting Cases\n",
      "The importing and exporting of cases takes place on the case dashboard, which you can reach by clicking on \"view all cases\" under the relevancy cases dropdown.\n",
      "Here you can import cases from a CSV file. You can also export to a CSV file all cases, just the cases you own, or just the cases shared with you. With Quepid, the data is always yours and it is easy to export should you need it.\n",
      "Managing Cases\n",
      "Also on the case dashboard, you can manage existing cases. You can see both the cases you own and those that have been shared with you.\n",
      "For each set of cases, you can filter the list by name and number of documents per page, as well as paginate through the pages if you have a larger number of cases. You can also click on the case name to open that case in the main Quepid dashboard.\n",
      "For each case, you have the ability to archive or rename. Cases should be archived when they are no longer considered active. You can still access archived cases using the \"view archived cases\" at the bottom of the page. You can also click on the case name to open that case in the main Quepid dashboard.\n",
      "Queries\n",
      "If you completed the \"Quick Start Wizard\", you may have already added a few queries. In this section, we will explore how you interact with queries and what actions can be taken.\n",
      "Adding Queries\n",
      "Adding additional queries is simple. Just type in your search term(s) and click \"Add query\" and Quepid will populate the results. A newly added query will display an empty score, the title, and the number of results found. Click on the arrow to expand a single result:\n",
      "Want to add a number of queries all at once? Just seperate them with a \";\", like so: \"star wars;star trek\" and click \"Add query\".\n",
      "Rating Queries\n",
      "Queries are rated through the dropdown to the left of each result. Clicking on the dropdown will bring up the rating scale based on the scorer you have selected. At this point, your scorer is likely still the default 1 to 10 scale.\n",
      "In addition to rating individual results, you can use the \"score all\" dropdown to rate all results the same for a query. As you rate each query, the average score for both the query and the case will be automatically updated.\n",
      "Explain Missing Documents\n",
      "You now know how to rate documents that appear in your top ten results. What happens if good documents you expected to be returned by a particular query are not even showing up?  You will need a way to indicate that more relevant documents, that do exist in the data, are missing from this desired results in this result set, and in an ideal future state of the world, would be found by this same query.  Here's how that is accomplished.\n",
      "Clicking on \"Explain Missing Documents\" will bring up a dialog box that initially shows you all of the documents that you have rated, but aren't being returned in your top ten results.  Here you can also find additonal documents to rate, just use the Lucene syntax to query for them. If you're looking for a specific document and you know the ID, that is the simplest way to locate it. Once you have rated the missing document, that rating is factored into the algorithm.\n",
      "Managing Queries\n",
      "In addition to rating queries, there are several other actions you can take. You can open the notes panel, where you can make comments on your query for yourself or another team member.  A great place to document the information need this query represents.\n",
      "You can also set a threshold for each query, a minimum score below which a warning icon will be displayed in the UI. Setting thresholds is an important method of managing benchmarks and goals, particularly as you are managing larger numbers of queries.\n",
      "You can also move queries to other cases, and finally, you can delete queries.\n",
      "Snapshots\n",
      "Another important step in the benchmarking process for Quepid is taking regular snapshots of your case over time and comparing them to ensure that relevancy is improving.\n",
      "Creating Snapshots\n",
      "When you are ready to take a snapshot, click on \"Create Snapshot\" in the case actions area to bring up the snapshot dialog box. From here, give your snapshot a descriptive name and click \"Take Snapshot.\" The current ratings for all of your queries are now saved.\n",
      "Comparing Snapshots\n",
      "Once you have made improvements to your search relevancy settings, you will want to judge your improvement. Click on the \"Compare snapshots\" to view your current results with a previous snapshot. You can now see the scores and results side-by-side for comparison.\n",
      "Scorers\n",
      "Scorers are run over all your queries to calculate how good the search results are according to the ratings you've made.\n",
      "You can specify whatever scale you want, from binary (0 or 1) to graded (0 to 3 or 1 to 10) scales.  Each scorer has slightly different behavior depending on if it handles graded versus binary scales.\n",
      "Classical Scorers Shipped with Quepid\n",
      "These are scorers that come with Quepid.  k refers the depth of the search results evaluated.  Learn more at Choosing Your Search Relevance Metric blog post.\n",
      "Scorer\n",
      "Scale\n",
      "Description\n",
      "Precision P@10\n",
      "binary (0 or 1)\n",
      "Precision measures the relevance of the entire results set.  It is the fraction of the documents retrieved that are relevant to the user's information need. Precision is scoped to measure only the top k results.\n",
      "Average Precision AP@10\n",
      "binary (0 or 1)\n",
      "Average Precision measures the relevance to a user scanning results sequentially.  It is similar to precision, but weights the ranking so that a 0 in rank 1 punishes the score more that a 0 in rank 5.\n",
      "Reciprocal RankRR@10\n",
      "binary (0 or 1)\n",
      "Reciprocal Rank measures how close to the number one position the first relevant document appears. It is a useful metric for known item search, such as a part number. A relevant document at position 1 scores 1, at positon 2 scores 1/2, and so forth.\n",
      "Cumulative Gain CG@10\n",
      "graded (0 to 3)\n",
      "Information gain from a results set.  It just totals up the grades for the top k results, regardless of ranking.\n",
      "Discount Cumulative Gain DCG@10\n",
      "graded (0 to 3)\n",
      "Builds on CG, however it includes positional weighting.   A 0 in Rank 1 punishes your score significantly more than Rank 5.\n",
      "normalized Discount Cumulative Gain nDCG@10\n",
      "graded (0 to 3)\n",
      "nDCG takes DCG and then measures it against a ideal relevance ranking yielding a score between 0 and 1.  Learn more about nDCG (including some gotchas) on the wiki.\n",
      "Selecting a Scorer\n",
      "By default, there are several scorers available. From the main dashboard screen, click \"Select scorer\" to choose from the available options. If you want to use a different scale for rating, click the \"Create New Scorer\" button. This will take you to the custom scorers page.\n",
      "Creating a Custom Scorer\n",
      "This page lets you change which scorer is your default scorer when you create new Cases.\n",
      "From the custom scorers page, click \"New Scorer.\" From here, you can name your new scorer, provide custom scoring logic in javascript, and select the scoring range.\n",
      "What can my code do?\n",
      "Your code validates the search results that came back. Below is an API available for you to work with:\n",
      "Function\n",
      "Description\n",
      "docAt(i)\n",
      "The document at the i'th location in the displayed search results from the search engine, including all fields displayed on Quepid. Empty object returned on no results.\n",
      "docExistsAt(i)\n",
      "Whether the i'th location has a document.\n",
      "eachDoc(function(doc, i) {}, num)\n",
      "Loop over docs. For each doc, call passed in function with a document, doc, and an index, i.\n",
      "You can pass in an optional num parameter to specify how many of the docs you want to use in the scoring (eg. to only use the top 5 results, pass in the number 5 as a second parameter. Default: 10\n",
      "eachRatedDoc(function(doc, i) {}, num)\n",
      "Loop over rated documents only.  Same arguments as eachDoc.\n",
      "Note: Needs rated documents loaded before usage, see refreshRatedDocs\n",
      "refreshRatedDocs(k)\n",
      "Refresh rated documents up to count k.  This method returns a promise and must be run before using eachRatedDoc.  You should call then() on the promise with a function that kicks off scoring.\n",
      "numFound()\n",
      "Solr has found this many results.\n",
      "numReturned()\n",
      "The total number of search results here.\n",
      "hasDocRating(i)\n",
      "True if a Quepid rating has been applied to this document.\n",
      "docRating(i)\n",
      "A document's rating for this query. This rating is relative to the scale you have chosen for your custom scorer.\n",
      "avgRating(num)\n",
      "The average rating of the returned documents. This rating is relative to the scale you have chosen for your custom scorer.\n",
      "You can pass in an optional num parameter to specify how many of the docs you want to use in the scoring (eg. to only use the top 5 results, pass in the number 5 as a second parameter. Default: 10\n",
      "avgRating100(num)\n",
      "The average rating of the returned documents. This rating is on a scale of 100 (i.e. the average score as a percentage).\n",
      "You can pass in an optional num parameter to specify how many of the docs you want to use in the scoring (eg. to only use the top 5 results, pass in the number 5 as a second parameter. Default: 10\n",
      "editDistanceFromBest(num)\n",
      "An edit distance from the best rated search results.\n",
      "You can pass in an optional num parameter to specify how many of the docs you want to use in the scoring (eg. to only use the top 5 results, pass in the number 5 as a second parameter. Default: 10\n",
      "setScore(number)\n",
      "sets the query's score to number and immediately exits\n",
      "pass()\n",
      "pass the test (score it 100), immediately exits\n",
      "fail()\n",
      "fail the test (score it 0), immediately exits\n",
      "assert(condition)\n",
      "continues if the condition is true, otherwise immediately fails the test and exits\n",
      "setScore(number)\n",
      "sets the query's score to number and immediately exits\n",
      "Managing Scorers\n",
      "After you have created scorers, you can manage them from this page. You can filter down the list of scorers by name or type. You can share a scorer with a team. You can edit the settings for a scorer. Finally, you can delete a scorer.\n",
      "Tuning Relevance\n",
      "To modify your search settings, open the relevancy tuning panel by clicking the \"tune relevance\" link in the case actions area.\n",
      "The default panel allows you to modify the Solr or Elasticsearch query parameters directly. Update your query here and click \"Rerun my searches\" to see the results update.\n",
      "Magic Variables\n",
      "\"Magic variables\" are, in other terms, placeholders. As you've probably already seen, you can add a list of queries that Quepid will automatically run against your search engine. And in the section above, you saw where you can specify what parameters are being sent along the call. But the trick is knowing how you can plug in the queries from the list you created into the parameters you've just built. This is where \"magic variables\" come in.\n",
      "There are 3 types of \"magic variables\". Here's a description of each:\n",
      "Query Magic Variable\n",
      "The query variable is represented by the #$query## string. Quepid will replace any occurrence of that pattern with the full query. This is the simplest, yet also the most important one of the \"magic variables\".\n",
      "Example:\n",
      "Let's consider you have a movies index that you are searching against with Solr as the search engine, and you've specified the following simple query parameters: q=#$query##.\n",
      "Let's also assume you've got the following queries that you've added in Quepid: \"marvel\" and \"dc comics\".\n",
      "Quepid will call your Solr instance for each of those queries by passing it in the following parameters: q=marvel and q=dc comics. So the #$query## magic variable got replaced by the query strings for each query you've added to the case.\n",
      "Curator Variables\n",
      "Curator variables are mostly for convenience. It allows you to modify the query parameters without having to rewrite the query directly. The way to define a curator variable is to add the following pattern to your query parameters: ##variableName##.\n",
      "They are used for numerical values only.\n",
      "The benefit of the curator vars is to adjust certain parameters and tweak them in an easy way until you reach the desired output, like a boost for example.\n",
      "Once you've defined the curator var in the query params, it will show up in the \"Variables\" tab of the tuning pane.\n",
      "Example:\n",
      "Continuing with the example from above, let's say you've progressed and added the following to our query params: q=#$query##&defType=edismax&qf=title^500 overview^10 tagline&tie=0.5.\n",
      "But you are still uncertain about the the boost values, so you replace the numbers with a curator var like so: q=#$query##&defType=edismax&qf=title^##titleBoost## overview^##overviewBoost## tagline&tie=0.5. Then in the \"Variables\" tab, you adjust the numbers as follows:\n",
      "Keyword Variables\n",
      "The query variable (#$query##) will be sufficient for 80% of the cases, but there are times when you need to break up the query send into different keywords that will be used in the query params in a different manner. This is where keyword variables come in. They are defined by the pattern #$keywordN## where N is an integer starting at 1.\n",
      "Quepid will take the query that would normally be represented by #$query## and split in on whitespace, then replace each occurrence of #$keywordN## by the appropriate keyword from the query.\n",
      "Example:\n",
      "Using the same example from above where you've added the following queries: \"marvel\" and \"dc comics\",\n",
      "let's assume that your query params looks like this now q=#$keyword1## where you only want to pass in the first word in the query to the search engine.\n",
      "In this case, the following params would be send to the search engine: q=marvel and q=dc (notice how it dropped the \"comics\" part of the second query).\n",
      "Of course you can imagine much more complex examples where the position of the word might affect the relevancy score of the results, or using permutations to come up with different phrases that might all be matched against. Example: \"Web site developer\" might be transformed in to \"Web\" + \"developer\" + \"Web developer\" + \"site developer\" + \"web site\", etc. And the results you return to your user would be the combination of all results that match all those permutations.\n",
      "Settings\n",
      "The settings panel allows you to modify the search configuration that you entered during the case's setup.\n",
      "History\n",
      "Each time you rerun your searches, a new \"try\" is generated in the search history. Clicking on a try will change the settings to that iteration, so you can move back and forth between different settings/configurations. You can also duplicate, rename or delete tries.\n",
      "Teams\n",
      "Clicking on the \"teams\" link in the main navigation will bring you to the page for managing teams.\n",
      "Creating Teams\n",
      "Clicking on the \"New\" button will allow you to add a team. Just provide a name and click \"create\" and the team will be added to the list of teams you own.\n",
      "Managing Teams\n",
      "From the teams page, you will see the summary information, including name, owner, number of members, and number of cases. You can change the name of the team by clicking \"edit\" or delete the team. To manage the details of the team, click on the name.\n",
      "On the team detail page, you can view existing members, add new members, and view cases shared with this team.\n",
      "Adding Users\n",
      "To add members to a team, just start typing their email address into \"add members\" box.\n",
      "If the email address isn't found, then it will change to a \"send invitation\" link.   Clicking it will send (if email is configured) an email to the person inviting them to join Quepid.  When they join via the invitation URL they will also join the team as well.\n",
      "While the invitation is waiting to be accepted, you can copy the invitation link to send to the invitee via your own email or chat tools via the \"clipboard\" icon.\n",
      "Troubleshooting\n",
      "In this section you will find common troubleshooting tips.  However, since browsers and search engines\n",
      "are constantly evolving, we also maintain a Troubleshooting Elasticsearch and Quepid and Troubleshooting Solr and Quepid wiki pages that you should consult for more up to date information.\n",
      "Additional tips on using Quepid are listed at Tips for Working with Quepid wiki page as well.\n",
      "Elasticsearch fields\n",
      "Version 5.0 of Elasticsearch has introduced some breaking changes. One of them that affects how Quepid works is the name of the params, namely the fields param was renamed to stored_fields. You can read the details here: https://www.elastic.co/guide/en/elasticsearch/reference/current/breaking_50_search_changes.html#_literal_fields_literal_parameter.\n",
      "In order for Quepid to be able to support both pre 5.0 and 5.x you need to specify which version you are on. By default, you are considered on version 5.0, so you won't have to do anything, but if you are on a version pre 5.0 you will need to update the Try settings.\n",
      "To update the version used, you just need to go to the Tune Relevance pane, and then switch to the Settings tab and then change the following:\n",
      "to any version lower than 5.0:\n",
      "and then rerun the search and any error you were seeing should be resolved.\n",
      "Quepid is built and maintained by the search experts at OpenSource Connections.\n",
      "Contact Us for your search consulting needs.\n",
      "For community support and discussion please join us in the #Quepid channel on Relevancy Slack at\n",
      "www.opensourceconnections.com/slack\n",
      "© OpenSource Connections 2020\n",
      "Home\n",
      "Why Quepid\n",
      "Tour\n",
      "Wiki\n",
      "Blog\n"
     ]
    }
   ],
   "source": [
    "## Extract plain text from html\n",
    "doc_text = extract_text_from(config.get(\"doc_url\",None))\n",
    "print(doc_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0cad5857",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 273, which is longer than the specified 225\n",
      "Created a chunk of size 270, which is longer than the specified 225\n",
      "Created a chunk of size 296, which is longer than the specified 225\n",
      "Created a chunk of size 286, which is longer than the specified 225\n",
      "Created a chunk of size 278, which is longer than the specified 225\n",
      "Created a chunk of size 288, which is longer than the specified 225\n",
      "Created a chunk of size 229, which is longer than the specified 225\n",
      "Created a chunk of size 248, which is longer than the specified 225\n",
      "Created a chunk of size 304, which is longer than the specified 225\n",
      "Created a chunk of size 268, which is longer than the specified 225\n",
      "Created a chunk of size 231, which is longer than the specified 225\n",
      "Created a chunk of size 437, which is longer than the specified 225\n",
      "Created a chunk of size 471, which is longer than the specified 225\n",
      "Created a chunk of size 256, which is longer than the specified 225\n",
      "Created a chunk of size 252, which is longer than the specified 225\n",
      "Created a chunk of size 262, which is longer than the specified 225\n",
      "Created a chunk of size 263, which is longer than the specified 225\n",
      "Created a chunk of size 249, which is longer than the specified 225\n",
      "Created a chunk of size 274, which is longer than the specified 225\n",
      "Created a chunk of size 234, which is longer than the specified 225\n",
      "Created a chunk of size 444, which is longer than the specified 225\n",
      "Created a chunk of size 237, which is longer than the specified 225\n",
      "Created a chunk of size 249, which is longer than the specified 225\n",
      "Created a chunk of size 270, which is longer than the specified 225\n",
      "Created a chunk of size 340, which is longer than the specified 225\n",
      "Created a chunk of size 471, which is longer than the specified 225\n",
      "Created a chunk of size 266, which is longer than the specified 225\n",
      "Created a chunk of size 250, which is longer than the specified 225\n",
      "Created a chunk of size 261, which is longer than the specified 225\n",
      "Created a chunk of size 351, which is longer than the specified 225\n",
      "Created a chunk of size 267, which is longer than the specified 225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits ['Documentation | Quepid\\nToggle navigation\\nDocs\\nTutorials\\nDocumentation\\nReturn to Quepid\\nBack to top\\nGetting Started', 'Return to Quepid\\nBack to top\\nGetting Started\\nWelcome to Quepid, your search relevancy toolkit. Getting started is easy and we will walk you through the concepts and features to get you setup and tuning relevance in no time.', 'Getting Started\\nWelcome to Quepid, your search relevancy toolkit. Getting started is easy and we will walk you through the concepts and features to get you setup and tuning relevance in no time.\\nCore Concepts', \"Core Concepts\\nFirst, let's walk through a few of the core concepts in Quepid.\\nCase:\", 'Case:\\nA case refers to all of the queries and relevance tuning settings for a single search engine. If you want to work with multiple instances of Solr or Elasticsearch, you must create a separate case for each one.\\nQuery:', 'Query:\\nWithin a case, queries are the keywords or other search criteria and their corresponding set of results that will be rated to determine the overall score of a case.\\nRating:', 'Ratings are the numerical values given to a result that indicates how relevant a particular search result is for the query. How each rating is interpreted depends on the scorer used for the query (or case), but usually the higher the number the more relevant the result is.', 'Result:\\nWithin a query are the individual results, which are rated to determine the cumulative score of a query. Sometimes results are also referred to as documents (or docs).\\nScorer:', 'Scorer:\\nA scorer refers to the scale used to rate query results. Quepid ships with several classical relevance scorers such as AP, RR, CG, DCG, and NDCG, as well as the ability to create custom scorers.\\nSnapshot:', 'Snapshot:\\nA snapshot is a capture of all your queries and ratings at a point in time. It is important to take regular snapshots of your case to use as benchmarks and ensure that your search relevancy is improving.\\nTeam:', 'Team:\\nA team refers to a group of individual users who can view and share cases and custom scorers.\\nTry:', \"A try is a saved iteration of a case's settings. Quepid is a developer tool and we expect developers to constantly tweak the settings, and sometimes you would want to go back to a previous iteration that had better results, so we've made that an integral part of Quepid.\", 'Creating an Account\\nAll you need is an email address to sign up for free.\\nQuick Start Wizard', 'Quick Start Wizard\\nThe \"Quick Start Wizard\", which launches on first login or any time you create a new case, will guide you through configuring your Solr or Elasticsearch instance and setting up your case.', 'The \"Quick Start Wizard\", which launches on first login or any time you create a new case, will guide you through configuring your Solr or Elasticsearch instance and setting up your case.\\nWhat you need to complete the wizard', 'What you need to complete the wizard\\nA Solr or Elasticsearch instance that is accessible from your browser\\nA list of desired fields from your search engine', 'A Solr or Elasticsearch instance that is accessible from your browser\\nA list of desired fields from your search engine\\n*Note: You can use the demo Solr or Elasticsearch instances provided to test Quepid\\nNaming Your Case', '*Note: You can use the demo Solr or Elasticsearch instances provided to test Quepid\\nNaming Your Case\\nFirst you will be required to enter a case name. Select something descriptive to distinguish between your cases.', 'Naming Your Case\\nFirst you will be required to enter a case name. Select something descriptive to distinguish between your cases.\\nConnecting your Search Engine', 'Quepid does not require any installation on your server. All you need to do is indicate which search engine you are working with, and provide the URL to your search engine. Currently Solr and Elasticsearch are supported.', 'You will be able to update this URL at any point in the settings. If you want to try Quepid out without providing your data, feel free to leave the default collection selected.\\nSelecting Fields to Display', 'Specify the title field to use for display purposes. This typically corresponds to the field you use as a clickable title in your application while displaying search results. The application provides an autosuggest list of potential fields, but you are free to use any field in you search engine.', 'The ID field is the only other required field to use Quepid. This should correspond to the unique identifier specified in your schema and should be a number or string, but not a URL. The ID is the only field stored in Quepid, so your sensitive data will remain securely on your servers.', 'You can select additional fields if you would like to see other data in your Quepid results. You can also specify images with the following syntax: thumb:field_name and video/audio with media:field_name.  Fields that have a value that starts with http will be turned into links.', 'Adding Queries', 'At this point the required setup steps are complete. You have the option to provide some initial queries, but you can also add queries at any time in the application. It is helpful to add at least one query here so that your results will be populated when the application initially loads.', \"Finishing the Wizard\\nThat's it! Quepid is configured and you can now proceed to exploring the Quepid interface.\\nExploring the Interface\", \"Exploring the Interface\\nNow that we have the basic setup out of the way, let's take a look at the major elements of the Quepid interface.\\n1. Relevancy Case Dropdown-\", '1. Relevancy Case Dropdown-\\nThe relevancy case dropdown is where you will select a case to work on and create new cases.\\n2. Case Information-', '2. Case Information-\\nThis area displays summary information of the current case, including case name, the average score of all queries, and the current try, or iteration in the relevancy dashboard history.\\n3. Case Actions-', '3. Case Actions-\\nThis area displays available actions for the current case, including selecting a scorer, creating and comparing snapshots, sharing, and toggling the relevancy tuning panel.\\n4. Queries-', '4. Queries-\\nHere you can add queries to the current case and view existing queries. Click on the arrow to the right to view and rate results for each individual query.\\n5. Relevancy Tuning Panel-', '5. Relevancy Tuning Panel-\\nThe relevancy tuning panel allows you to modify search relevancy settings and tune your searches. This panel is closed by default and can be opened in the case actions section.\\n6. Quepid Support-', 'All pages of the Quepid application include the Quepid support chat window. Click the icon in the bottom right hand of your screen and our search relevancy experts will assist you with any issues or questions you may have.', 'Cases\\nIn this section, we will review working with cases in Quepid. If you\\'ve logged into the application and gone through the \"Quick Start Wizard\", then you have already created your first case.\\nCreating a New Case', 'Creating a New Case\\nIf you would like to create an addition case, you can do so under the Relevancy Cases dropdown. Clicking on \"Create a case\" will launch the \"Quick Start Wizard\" and walk you through creating another case.', 'Importing/Exporting Cases\\nThe importing and exporting of cases takes place on the case dashboard, which you can reach by clicking on \"view all cases\" under the relevancy cases dropdown.', 'Here you can import cases from a CSV file. You can also export to a CSV file all cases, just the cases you own, or just the cases shared with you. With Quepid, the data is always yours and it is easy to export should you need it.', 'Managing Cases\\nAlso on the case dashboard, you can manage existing cases. You can see both the cases you own and those that have been shared with you.', 'For each set of cases, you can filter the list by name and number of documents per page, as well as paginate through the pages if you have a larger number of cases. You can also click on the case name to open that case in the main Quepid dashboard.', 'For each case, you have the ability to archive or rename. Cases should be archived when they are no longer considered active. You can still access archived cases using the \"view archived cases\" at the bottom of the page. You can also click on the case name to open that case in the main Quepid dashboard.', 'Queries\\nIf you completed the \"Quick Start Wizard\", you may have already added a few queries. In this section, we will explore how you interact with queries and what actions can be taken.\\nAdding Queries', 'Adding additional queries is simple. Just type in your search term(s) and click \"Add query\" and Quepid will populate the results. A newly added query will display an empty score, the title, and the number of results found. Click on the arrow to expand a single result:', 'Want to add a number of queries all at once? Just seperate them with a \";\", like so: \"star wars;star trek\" and click \"Add query\".\\nRating Queries', 'Queries are rated through the dropdown to the left of each result. Clicking on the dropdown will bring up the rating scale based on the scorer you have selected. At this point, your scorer is likely still the default 1 to 10 scale.', 'In addition to rating individual results, you can use the \"score all\" dropdown to rate all results the same for a query. As you rate each query, the average score for both the query and the case will be automatically updated.', 'Explain Missing Documents', \"You now know how to rate documents that appear in your top ten results. What happens if good documents you expected to be returned by a particular query are not even showing up?  You will need a way to indicate that more relevant documents, that do exist in the data, are missing from this desired results in this result set, and in an ideal future state of the world, would be found by this same query.  Here's how that is accomplished.\", 'Clicking on \"Explain Missing Documents\" will bring up a dialog box that initially shows you all of the documents that you have rated, but aren\\'t being returned in your top ten results.  Here you can also find additonal documents to rate, just use the Lucene syntax to query for them. If you\\'re looking for a specific document and you know the ID, that is the simplest way to locate it. Once you have rated the missing document, that rating is factored into the algorithm.', 'Managing Queries', 'In addition to rating queries, there are several other actions you can take. You can open the notes panel, where you can make comments on your query for yourself or another team member.  A great place to document the information need this query represents.', 'You can also set a threshold for each query, a minimum score below which a warning icon will be displayed in the UI. Setting thresholds is an important method of managing benchmarks and goals, particularly as you are managing larger numbers of queries.', 'You can also move queries to other cases, and finally, you can delete queries.\\nSnapshots', 'Snapshots\\nAnother important step in the benchmarking process for Quepid is taking regular snapshots of your case over time and comparing them to ensure that relevancy is improving.\\nCreating Snapshots', 'When you are ready to take a snapshot, click on \"Create Snapshot\" in the case actions area to bring up the snapshot dialog box. From here, give your snapshot a descriptive name and click \"Take Snapshot.\" The current ratings for all of your queries are now saved.', 'Comparing Snapshots', 'Once you have made improvements to your search relevancy settings, you will want to judge your improvement. Click on the \"Compare snapshots\" to view your current results with a previous snapshot. You can now see the scores and results side-by-side for comparison.', \"Scorers\\nScorers are run over all your queries to calculate how good the search results are according to the ratings you've made.\", 'You can specify whatever scale you want, from binary (0 or 1) to graded (0 to 3 or 1 to 10) scales.  Each scorer has slightly different behavior depending on if it handles graded versus binary scales.', 'Classical Scorers Shipped with Quepid\\nThese are scorers that come with Quepid.  k refers the depth of the search results evaluated.  Learn more at Choosing Your Search Relevance Metric blog post.\\nScorer\\nScale\\nDescription', 'These are scorers that come with Quepid.  k refers the depth of the search results evaluated.  Learn more at Choosing Your Search Relevance Metric blog post.\\nScorer\\nScale\\nDescription\\nPrecision P@10\\nbinary (0 or 1)', \"Precision measures the relevance of the entire results set.  It is the fraction of the documents retrieved that are relevant to the user's information need. Precision is scoped to measure only the top k results.\", 'Average Precision AP@10\\nbinary (0 or 1)', 'binary (0 or 1)\\nAverage Precision measures the relevance to a user scanning results sequentially.  It is similar to precision, but weights the ranking so that a 0 in rank 1 punishes the score more that a 0 in rank 5.', 'Average Precision measures the relevance to a user scanning results sequentially.  It is similar to precision, but weights the ranking so that a 0 in rank 1 punishes the score more that a 0 in rank 5.\\nReciprocal RankRR@10', 'Reciprocal RankRR@10\\nbinary (0 or 1)', 'Reciprocal Rank measures how close to the number one position the first relevant document appears. It is a useful metric for known item search, such as a part number. A relevant document at position 1 scores 1, at positon 2 scores 1/2, and so forth.', 'Cumulative Gain CG@10\\ngraded (0 to 3)\\nInformation gain from a results set.  It just totals up the grades for the top k results, regardless of ranking.\\nDiscount Cumulative Gain DCG@10\\ngraded (0 to 3)', 'Discount Cumulative Gain DCG@10\\ngraded (0 to 3)\\nBuilds on CG, however it includes positional weighting.   A 0 in Rank 1 punishes your score significantly more than Rank 5.\\nnormalized Discount Cumulative Gain nDCG@10', 'graded (0 to 3)\\nBuilds on CG, however it includes positional weighting.   A 0 in Rank 1 punishes your score significantly more than Rank 5.\\nnormalized Discount Cumulative Gain nDCG@10\\ngraded (0 to 3)', 'normalized Discount Cumulative Gain nDCG@10\\ngraded (0 to 3)\\nnDCG takes DCG and then measures it against a ideal relevance ranking yielding a score between 0 and 1.  Learn more about nDCG (including some gotchas) on the wiki.', 'graded (0 to 3)\\nnDCG takes DCG and then measures it against a ideal relevance ranking yielding a score between 0 and 1.  Learn more about nDCG (including some gotchas) on the wiki.\\nSelecting a Scorer', 'By default, there are several scorers available. From the main dashboard screen, click \"Select scorer\" to choose from the available options. If you want to use a different scale for rating, click the \"Create New Scorer\" button. This will take you to the custom scorers page.', 'Creating a Custom Scorer\\nThis page lets you change which scorer is your default scorer when you create new Cases.', 'From the custom scorers page, click \"New Scorer.\" From here, you can name your new scorer, provide custom scoring logic in javascript, and select the scoring range.\\nWhat can my code do?', 'What can my code do?\\nYour code validates the search results that came back. Below is an API available for you to work with:\\nFunction\\nDescription\\ndocAt(i)', \"Function\\nDescription\\ndocAt(i)\\nThe document at the i'th location in the displayed search results from the search engine, including all fields displayed on Quepid. Empty object returned on no results.\\ndocExistsAt(i)\", \"The document at the i'th location in the displayed search results from the search engine, including all fields displayed on Quepid. Empty object returned on no results.\\ndocExistsAt(i)\\nWhether the i'th location has a document.\", \"docExistsAt(i)\\nWhether the i'th location has a document.\\neachDoc(function(doc, i) {}, num)\\nLoop over docs. For each doc, call passed in function with a document, doc, and an index, i.\", 'You can pass in an optional num parameter to specify how many of the docs you want to use in the scoring (eg. to only use the top 5 results, pass in the number 5 as a second parameter. Default: 10', 'eachRatedDoc(function(doc, i) {}, num)\\nLoop over rated documents only.  Same arguments as eachDoc.\\nNote: Needs rated documents loaded before usage, see refreshRatedDocs\\nrefreshRatedDocs(k)', 'refreshRatedDocs(k)\\nRefresh rated documents up to count k.  This method returns a promise and must be run before using eachRatedDoc.  You should call then() on the promise with a function that kicks off scoring.\\nnumFound()', 'numFound()\\nSolr has found this many results.\\nnumReturned()\\nThe total number of search results here.\\nhasDocRating(i)\\nTrue if a Quepid rating has been applied to this document.\\ndocRating(i)', \"hasDocRating(i)\\nTrue if a Quepid rating has been applied to this document.\\ndocRating(i)\\nA document's rating for this query. This rating is relative to the scale you have chosen for your custom scorer.\\navgRating(num)\", 'avgRating(num)\\nThe average rating of the returned documents. This rating is relative to the scale you have chosen for your custom scorer.', 'You can pass in an optional num parameter to specify how many of the docs you want to use in the scoring (eg. to only use the top 5 results, pass in the number 5 as a second parameter. Default: 10\\navgRating100(num)', 'avgRating100(num)\\nThe average rating of the returned documents. This rating is on a scale of 100 (i.e. the average score as a percentage).', 'You can pass in an optional num parameter to specify how many of the docs you want to use in the scoring (eg. to only use the top 5 results, pass in the number 5 as a second parameter. Default: 10\\neditDistanceFromBest(num)', 'editDistanceFromBest(num)\\nAn edit distance from the best rated search results.', 'You can pass in an optional num parameter to specify how many of the docs you want to use in the scoring (eg. to only use the top 5 results, pass in the number 5 as a second parameter. Default: 10\\nsetScore(number)', \"setScore(number)\\nsets the query's score to number and immediately exits\\npass()\\npass the test (score it 100), immediately exits\\nfail()\\nfail the test (score it 0), immediately exits\\nassert(condition)\", 'pass()\\npass the test (score it 100), immediately exits\\nfail()\\nfail the test (score it 0), immediately exits\\nassert(condition)\\ncontinues if the condition is true, otherwise immediately fails the test and exits\\nsetScore(number)', \"fail()\\nfail the test (score it 0), immediately exits\\nassert(condition)\\ncontinues if the condition is true, otherwise immediately fails the test and exits\\nsetScore(number)\\nsets the query's score to number and immediately exits\", \"assert(condition)\\ncontinues if the condition is true, otherwise immediately fails the test and exits\\nsetScore(number)\\nsets the query's score to number and immediately exits\\nManaging Scorers\", 'After you have created scorers, you can manage them from this page. You can filter down the list of scorers by name or type. You can share a scorer with a team. You can edit the settings for a scorer. Finally, you can delete a scorer.', 'Tuning Relevance\\nTo modify your search settings, open the relevancy tuning panel by clicking the \"tune relevance\" link in the case actions area.', 'The default panel allows you to modify the Solr or Elasticsearch query parameters directly. Update your query here and click \"Rerun my searches\" to see the results update.\\nMagic Variables', '\"Magic variables\" are, in other terms, placeholders. As you\\'ve probably already seen, you can add a list of queries that Quepid will automatically run against your search engine. And in the section above, you saw where you can specify what parameters are being sent along the call. But the trick is knowing how you can plug in the queries from the list you created into the parameters you\\'ve just built. This is where \"magic variables\" come in.', 'There are 3 types of \"magic variables\". Here\\'s a description of each:\\nQuery Magic Variable', 'The query variable is represented by the #$query## string. Quepid will replace any occurrence of that pattern with the full query. This is the simplest, yet also the most important one of the \"magic variables\".\\nExample:', \"Example:\\nLet's consider you have a movies index that you are searching against with Solr as the search engine, and you've specified the following simple query parameters: q=#$query##.\", 'Let\\'s also assume you\\'ve got the following queries that you\\'ve added in Quepid: \"marvel\" and \"dc comics\".', \"Quepid will call your Solr instance for each of those queries by passing it in the following parameters: q=marvel and q=dc comics. So the #$query## magic variable got replaced by the query strings for each query you've added to the case.\", 'Curator Variables', 'Curator variables are mostly for convenience. It allows you to modify the query parameters without having to rewrite the query directly. The way to define a curator variable is to add the following pattern to your query parameters: ##variableName##.', 'They are used for numerical values only.\\nThe benefit of the curator vars is to adjust certain parameters and tweak them in an easy way until you reach the desired output, like a boost for example.', 'Once you\\'ve defined the curator var in the query params, it will show up in the \"Variables\" tab of the tuning pane.\\nExample:', \"Example:\\nContinuing with the example from above, let's say you've progressed and added the following to our query params: q=#$query##&defType=edismax&qf=title^500 overview^10 tagline&tie=0.5.\", 'But you are still uncertain about the the boost values, so you replace the numbers with a curator var like so: q=#$query##&defType=edismax&qf=title^##titleBoost## overview^##overviewBoost## tagline&tie=0.5. Then in the \"Variables\" tab, you adjust the numbers as follows:', 'Keyword Variables', 'The query variable (#$query##) will be sufficient for 80% of the cases, but there are times when you need to break up the query send into different keywords that will be used in the query params in a different manner. This is where keyword variables come in. They are defined by the pattern #$keywordN## where N is an integer starting at 1.', 'Quepid will take the query that would normally be represented by #$query## and split in on whitespace, then replace each occurrence of #$keywordN## by the appropriate keyword from the query.\\nExample:', 'Example:\\nUsing the same example from above where you\\'ve added the following queries: \"marvel\" and \"dc comics\",', \"let's assume that your query params looks like this now q=#$keyword1## where you only want to pass in the first word in the query to the search engine.\", 'In this case, the following params would be send to the search engine: q=marvel and q=dc (notice how it dropped the \"comics\" part of the second query).', 'Of course you can imagine much more complex examples where the position of the word might affect the relevancy score of the results, or using permutations to come up with different phrases that might all be matched against. Example: \"Web site developer\" might be transformed in to \"Web\" + \"developer\" + \"Web developer\" + \"site developer\" + \"web site\", etc. And the results you return to your user would be the combination of all results that match all those permutations.', \"Settings\\nThe settings panel allows you to modify the search configuration that you entered during the case's setup.\\nHistory\", 'Each time you rerun your searches, a new \"try\" is generated in the search history. Clicking on a try will change the settings to that iteration, so you can move back and forth between different settings/configurations. You can also duplicate, rename or delete tries.', 'Teams\\nClicking on the \"teams\" link in the main navigation will bring you to the page for managing teams.\\nCreating Teams', 'Creating Teams\\nClicking on the \"New\" button will allow you to add a team. Just provide a name and click \"create\" and the team will be added to the list of teams you own.\\nManaging Teams', 'From the teams page, you will see the summary information, including name, owner, number of members, and number of cases. You can change the name of the team by clicking \"edit\" or delete the team. To manage the details of the team, click on the name.', 'On the team detail page, you can view existing members, add new members, and view cases shared with this team.\\nAdding Users\\nTo add members to a team, just start typing their email address into \"add members\" box.', 'If the email address isn\\'t found, then it will change to a \"send invitation\" link.   Clicking it will send (if email is configured) an email to the person inviting them to join Quepid.  When they join via the invitation URL they will also join the team as well.', 'While the invitation is waiting to be accepted, you can copy the invitation link to send to the invitee via your own email or chat tools via the \"clipboard\" icon.\\nTroubleshooting', 'Troubleshooting\\nIn this section you will find common troubleshooting tips.  However, since browsers and search engines', 'are constantly evolving, we also maintain a Troubleshooting Elasticsearch and Quepid and Troubleshooting Solr and Quepid wiki pages that you should consult for more up to date information.', 'Additional tips on using Quepid are listed at Tips for Working with Quepid wiki page as well.\\nElasticsearch fields', 'Version 5.0 of Elasticsearch has introduced some breaking changes. One of them that affects how Quepid works is the name of the params, namely the fields param was renamed to stored_fields. You can read the details here: https://www.elastic.co/guide/en/elasticsearch/reference/current/breaking_50_search_changes.html#_literal_fields_literal_parameter.', \"In order for Quepid to be able to support both pre 5.0 and 5.x you need to specify which version you are on. By default, you are considered on version 5.0, so you won't have to do anything, but if you are on a version pre 5.0 you will need to update the Try settings.\", 'To update the version used, you just need to go to the Tune Relevance pane, and then switch to the Settings tab and then change the following:\\nto any version lower than 5.0:', 'to any version lower than 5.0:\\nand then rerun the search and any error you were seeing should be resolved.\\nQuepid is built and maintained by the search experts at OpenSource Connections.', 'and then rerun the search and any error you were seeing should be resolved.\\nQuepid is built and maintained by the search experts at OpenSource Connections.\\nContact Us for your search consulting needs.', 'Quepid is built and maintained by the search experts at OpenSource Connections.\\nContact Us for your search consulting needs.\\nFor community support and discussion please join us in the #Quepid channel on Relevancy Slack at', 'Contact Us for your search consulting needs.\\nFor community support and discussion please join us in the #Quepid channel on Relevancy Slack at\\nwww.opensourceconnections.com/slack\\n© OpenSource Connections 2020\\nHome\\nWhy Quepid', 'For community support and discussion please join us in the #Quepid channel on Relevancy Slack at\\nwww.opensourceconnections.com/slack\\n© OpenSource Connections 2020\\nHome\\nWhy Quepid\\nTour\\nWiki\\nBlog'] into 135 chunks\n"
     ]
    }
   ],
   "source": [
    "## We use langchain here to split text into chunks of 225 tokens each\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=225, separator=\"\\n\")\n",
    "#docs = text_splitter.split_documents(doc_text)\n",
    "splits = text_splitter.split_text(doc_text)\n",
    "print(f\"Splits {splits} into {len(splits)} chunks\")\n",
    "##Print splitted chunks and number of chunks \"into 135 chunks\" in our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b81ad094",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving text + embedding in Chroma DB \n",
    "persist_directory = config.get(\"persist_directory\",None)\n",
    "if persist_directory and os.path.exists(persist_directory):\n",
    "    vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)\n",
    "else:\n",
    "    vectordb = Chroma.from_texts(texts=splits, embedding=embedding, persist_directory=persist_directory)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a9e53d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retrieving data from using langchain RetrievalQA\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\":4})\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\",retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c7675581",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To get ratings for search results, you can follow these steps:\n",
      "\n",
      "1. Open the search results page in the search engine or platform you are using.\n",
      "2. Look for a dropdown or menu option next to each search result. This dropdown usually contains a rating scale.\n",
      "3. Click on the dropdown to bring up the rating scale.\n",
      "4. Select the appropriate rating for each search result based on its relevance to your query. The rating scale is typically a numerical scale, such as 1 to 10, where a higher number indicates a more relevant result.\n",
      "5. Repeat this process for each search result on the page.\n",
      "6. Some platforms also provide an option to \"score all\" results the same for a particular query. This means you can assign the same rating to all results with a single click.\n",
      "7. As you rate each search result, the average score for both the query and the case will be automatically updated.\n",
      "\n",
      "Note that the specific steps may vary depending on the search engine or platform you are using.\n"
     ]
    }
   ],
   "source": [
    "question = \"How to get ratings?\"\n",
    "\n",
    "answer = qa.run(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "97773388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, you can use Quepid with Elasticsearch. Quepid supports both Solr and Elasticsearch as search engines. All you need to do is indicate that you are working with Elasticsearch and provide the URL to your Elasticsearch instance.\n"
     ]
    }
   ],
   "source": [
    "question = \"can i use quepid with elasticsearch?\"\n",
    "\n",
    "answer = qa.run(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8f24a54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No, currently Quepid only supports Solr and Elasticsearch as search engines. Algolia is not supported.\n"
     ]
    }
   ],
   "source": [
    "question = \"can i use quepid with algolia?\"\n",
    "\n",
    "answer = qa.run(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4639f97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, you can have snapshots in Quepid. Snapshots allow you to save the current ratings for all of your queries at a specific point in time. This allows you to compare snapshots over time and track the improvement in relevancy. To create a snapshot, click on \"Create Snapshot\" in the case actions area, give your snapshot a descriptive name, and click \"Take Snapshot.\"\n"
     ]
    }
   ],
   "source": [
    "question = \"can i have snapshots in quepid?\"\n",
    "\n",
    "answer = qa.run(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0a9d7f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quepid can help you tune the relevance of your search engine by providing a toolkit specifically designed for this purpose. It allows you to add queries and view the results, and then modify the search relevancy settings to improve the accuracy and effectiveness of your search results. The relevancy tuning panel in Quepid allows you to make adjustments and fine-tune the relevancy of your search engine.\n"
     ]
    }
   ],
   "source": [
    "question = \"How can quepid help me tuning relevance of my search engine\"\n",
    "\n",
    "answer = qa.run(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff00f6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
